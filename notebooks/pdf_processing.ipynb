{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import urllib.request\n",
    "from collections import OrderedDict\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_covid_mobility():\n",
    "    \n",
    "    \"\"\"\n",
    "    Checks for recent google mobility data and if new data available downloads pdf.\n",
    "    Saves all pdfs in '../data/raw' directory under sub directory with date of report\n",
    "    \n",
    "    outputs: boolean: True if new files downloaded\n",
    "            directory: directory path where new files were saved \n",
    "            \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists('../data'):\n",
    "        os.makedirs('../data')\n",
    "    \n",
    "    if not os.path.exists('../data/raw'):\n",
    "        os.makedirs('../data/raw')\n",
    "        \n",
    "    #use requests to get the mobility site\n",
    "    url = 'https://www.google.com/covid19/mobility/'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    #Use Beautiful Soup to parse the site for html \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    #find the download links for each country and region\n",
    "    html = soup.findAll('a', {\"class\":\"download-link\"})\n",
    "    \n",
    "    new_files = False\n",
    "    \n",
    "    #check the latest date of links\n",
    "    date_index = html[0]['href'].find('2020')\n",
    "    date = html[0]['href'][date_index:date_index+10]\n",
    "    if not os.path.exists(f'../data/raw/{date}'):\n",
    "        os.makedirs(f'../data/raw/{date}')\n",
    "        \n",
    "    #download all PDFs\n",
    "    for tag in html:\n",
    "        link = tag['href']\n",
    "        file_name = link[link.find('2020'):]      #file name based on download url which always starts with 2020 date\n",
    "        path = f\"../data/raw/{date}/{file_name}\"\n",
    "        #check to see if Google has uploaded new data\n",
    "        if not os.path.isfile(path):\n",
    "            new_files = True\n",
    "            urllib.request.urlretrieve(link, path)\n",
    "            print(f'new file found: {file_name}')\n",
    "            \n",
    "    directory = f'../data/raw/{date}'\n",
    "    if new_files == True:\n",
    "        directory = f'../data/raw/{date}'\n",
    "        print(f'New files downloaded for {date}')\n",
    "        status = True\n",
    "        return status, directory\n",
    "    \n",
    "    if new_files == False:\n",
    "        print('No new files')\n",
    "        status = False\n",
    "        return status, directory\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covid_report_to_text(pdf_path):\n",
    "    \"\"\"\n",
    "    takes pdf and extracts text into string\n",
    "    pdf_path: file path of the pdf to be converted\n",
    "    returns: string of text\n",
    "    \"\"\" \n",
    "    output_string = io.StringIO()\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        for page in PDFPage.get_pages(pdf_file):\n",
    "            interpreter.process_page(page)\n",
    "\n",
    "        text = output_string.getvalue()\n",
    "\n",
    "    device.close()\n",
    "    output_string.close()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_main_region(text):\n",
    "    \"\"\"\n",
    "    Parses pdf text to find stats for macro region (country level\n",
    "    or state level)\n",
    "    \n",
    "    text input is pdf converted into string\n",
    "    returns: dictionary of data, index of last category scraped\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    data = OrderedDict()\n",
    "    categories=['Retail & recreation', \n",
    "                'Grocery & pharmacy',\n",
    "                'Parks', 'Transit stations',\n",
    "                'Workplaces', 'Residential']\n",
    "\n",
    "    #find macro area and macro level stats\n",
    "    country_state = text.split('\\n\\n')[1].split('  ')[0]\n",
    "    data['Region']=[country_state]\n",
    "    for cat in categories:\n",
    "        index = text.find(cat)+len(cat)\n",
    "        if text[index]!=' ':\n",
    "            data[cat] = data.get(cat,[])+[int(text[index:index+text[index:].find('%')])]\n",
    "        else:                                   \n",
    "            data[cat] = data.get(cat, []) + [None]\n",
    "\n",
    "    last_cat_index = text.find(categories[-1])\n",
    "    \n",
    "    return data, last_cat_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sub_regions(text, data, last_cat_index):\n",
    "    \"\"\"\n",
    "    Parses pulls out stats for subregions\n",
    "    \n",
    "    Takes: text = string converted pdf\n",
    "            data = ordered dictionary from parse_main_region()\n",
    "            index = ending index number from parse_main_region()\n",
    "    \n",
    "    returns: data: ordered dictionary with main region and sub region stats\n",
    "    \"\"\"\n",
    "    \n",
    "    #clean the text for easier parsing\n",
    "    text_clean = text.replace('\\n+80%\\n','').replace('\\n-80%\\n','').replace('\\n+40%\\n','').replace('\\n-40%\\n','')\n",
    "    text_clean = text_clean.replace('Not enough data for this date','N/A')\n",
    "    text_clean = text_clean.replace('N/A:',\"\")\n",
    "    text_clean = text_clean.replace('*','').replace('Baseline','')\n",
    "    text_clean = text_clean.replace('N/A','N/A%')\n",
    "\n",
    "    #define categories to loop through\n",
    "    categories=['Retail & recreation', \n",
    "                    'Grocery & pharmacy',\n",
    "                    'Parks', 'Transit stations',\n",
    "                    'Workplaces', 'Residential']\n",
    "\n",
    "    counter = text_clean.find('Retail', last_cat_index) #counter to find the end of the sub regions\n",
    "    while counter >0:\n",
    "\n",
    "        #find the sub region based on location of next \"Retail\" from super region's end\n",
    "        region_end_index = text_easier.find('Retail', last_cat_index)-2\n",
    "        region_beg_index = text_easier.rfind('\\n',0,region_end_index)\n",
    "        region = text_easier[region_beg_index:region_end_index].replace('\\n','').replace('\\x0c','')\n",
    "        data['Region']+=[region]\n",
    "\n",
    "        #find 6 numbers or n/a following the location of the subregion\n",
    "        stat_ind = region_end_index\n",
    "        for cat in categories:\n",
    "            stat_ind = text_easier.find('%',stat_ind+1)\n",
    "            stat = text_easier[stat_ind-3:stat_ind].strip('\\n')\n",
    "            if stat == 'N/A':\n",
    "                data[cat]+=[None]\n",
    "            else:\n",
    "                stat = int(stat)\n",
    "                data[cat]+=[stat]\n",
    "            stat_ind+=1\n",
    "\n",
    "        #reset index cursors and counter\n",
    "        last_cat_index = stat_ind\n",
    "        counter = text_easier.find('Retail', last_cat_index)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn dictionary into pandas dataframe \n",
    "\n",
    "def dict_to_masterdf(master_df, data):\n",
    "    temp_df = pd.DataFrame(data)\n",
    "    master_df = master_df.append(temp_df)\n",
    "    return master_df\n",
    "\n",
    "def df_to_csv(df, file_name, directory):\n",
    "    processed_date = directory[-10:]\n",
    "    if not os.path.exists(f'../data/processed/{processed_date}'):\n",
    "        os.makedirs(f'../data/processed/{processed_date}')\n",
    "    df.to_csv (f'../data/processed/{processed_date}/{file_name}_{processed_date}.csv', index = False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_dict_to_masterdf(master_df, data):\n",
    "    \"\"\"\n",
    "    Drops the super-region level data so every entry is sub-region level\n",
    "    \n",
    "    Input: master_df: likely blank df\n",
    "            data: ordered dict of parsed stats\n",
    "            \n",
    "    Returns: df with every state's data appended \n",
    "    \"\"\"\n",
    "    if len(data['Region'])>1:\n",
    "        temp_df = pd.DataFrame(data)\n",
    "        temp_df['State']=data['Region'][0]\n",
    "        temp_df.drop(index=0, inplace = True)\n",
    "        master_df = master_df.append(temp_df)\n",
    "        return master_df\n",
    "    else:\n",
    "        temp_df = pd.DataFrame(data)\n",
    "        temp_df['State']=data['Region'][0]\n",
    "        master_df = master_df.append(temp_df)\n",
    "        return master_df\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_US_state_report(directory):\n",
    "    \"\"\"\n",
    "    Produces csv of county level data of all US States.\n",
    "    Input: directory path of pdf files\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Building US county level report')\n",
    "    us_list = [file for file in os.listdir(directory) if '_US_' in file]\n",
    "    us_list.remove(f'{us_list[0][:10]}_US_Mobility_Report_en.pdf') #drop the nation wide stats\n",
    "    master_df = pd.DataFrame()\n",
    "    for file in us_list:\n",
    "        text = covid_report_to_text(f'{directory}/{file}')\n",
    "        data, last_cat_index = parse_main_region(text)\n",
    "        data = parse_sub_regions(text, data, last_cat_index)\n",
    "        master_df = region_dict_to_masterdf(master_df, data)\n",
    "    df_to_csv(master_df, 'United_States_county', directory)\n",
    "    \n",
    "    print('US county level report done')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_global_covid_report(directory):\n",
    "    \"\"\"\n",
    "    Produces csv of nation level data for world.\n",
    "    Input: directory path of pdf files\n",
    "    \"\"\"\n",
    "    print('Building global report')\n",
    "    world_list = [file for file in os.listdir(directory) if '_US_' not in file]\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\"_US_Mobility_Report_en.pdf\"):\n",
    "            world_list.append(file)\n",
    "    master_df = pd.DataFrame()\n",
    "    for file in world_list:\n",
    "        text = covid_report_to_text(f'{directory}/{file}')\n",
    "        data, last_cat_index = parse_main_region(text)\n",
    "        master_df = dict_to_masterdf(master_df, data)\n",
    "    df_to_csv(master_df, 'World', directory)\n",
    "    print('Global report done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regionlevel_covid_report(directory):\n",
    "    \"\"\"\n",
    "    Produces individual csvs of region level data for any country with sub region data available.\n",
    "    Input: directory path of pdf files\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Building region level report')\n",
    "    world_list = [file for file in os.listdir(directory) if '_US_' not in file] #filter out US county level data\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\"_US_Mobility_Report_en.pdf\"): #put US state level data back in\n",
    "            world_list.append(file)\n",
    "    \n",
    "    \n",
    "    for file in world_list:\n",
    "        text = covid_report_to_text(f'{directory}/{file}')\n",
    "        data, last_cat_index = parse_main_region(text)\n",
    "        data = parse_sub_regions(text, data, last_cat_index)\n",
    "        if len(data['Region'])>1:\n",
    "            df = pd.DataFrame()\n",
    "            df = region_dict_to_masterdf(df, data)\n",
    "            df_to_csv(df, data['Region'][0].replace(' ','_'), directory)\n",
    "    print('region level done')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    status, directory = scrape_covid_mobility()\n",
    "    if status == True:\n",
    "        build_US_state_report(directory)\n",
    "        build_global_covid_report(directory)\n",
    "        build_regionlevel_covid_report(directory)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new files\n"
     ]
    }
   ],
   "source": [
    "run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (covid-env)",
   "language": "python",
   "name": "covid-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
